{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Wavenet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "# load the dataset\n",
    "with open(file=\"datasets/names.txt\", mode=\"r\") as namestxt:\n",
    "    words = namestxt.read().splitlines() \n",
    "    \n",
    "# build a dictionary for the characters\n",
    "chars = [\".\"] + sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "# shuffle the words\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # lengths of sequence of token \n",
    "def build_dataset(words, train=0.8, dev=0.1, test=0.1): # helper function to create datasets\n",
    "\tif train + test + dev != 1:\n",
    "\t\treturn ValueError\n",
    "\t\n",
    "\tX, Y = [], []\n",
    "\tfor word in words:\n",
    "\t\tcontext = [0]*block_size\n",
    "\t\tfor char in word + \".\":\n",
    "\t\t\tix = stoi[char]\n",
    "\t\t\tX.append(context)\n",
    "\t\t\tY.append(ix)\n",
    "\t\t\tcontext = context[1:]+[ix]\n",
    "\tX = torch.tensor(X)\n",
    "\tY = torch.tensor(Y)\n",
    "\n",
    "\tn1 = int(train*(len(words)))\n",
    "\tn2 = int((train + dev)*(len(words)))\n",
    "\n",
    "\tXtr, Ytr = X[:n1], Y[:n1]\n",
    "\tXdev, Ydev = X[n1:n2], Y[n1:n2]\n",
    "\tXtest, Ytest = X[n2:], Y[n2:]\n",
    "\n",
    "\treturn Xtr, Ytr, Xdev, Ydev, Xtest, Ytest\n",
    "\n",
    "Xtr, Ytr, Xdev, Ydev, Xtest, Ytest = build_dataset(words=words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... ---> y\n",
      "..y ---> u\n",
      ".yu ---> h\n",
      "yuh ---> e\n",
      "uhe ---> n\n",
      "hen ---> g\n",
      "eng ---> .\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(Xtr[:7], Ytr[:7]):\n",
    "\tprint(\"\".join(itos[ix.item()] for ix in x), \"--->\", itos[y.item()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some Layers Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
