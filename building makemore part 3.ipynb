{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 2147483647\n",
    "\n",
    "# read all the names in a python list\n",
    "with open(file=\"datasets/names.txt\", mode=\"r\") as namesTxt: \n",
    "    # names = [line[:-1] for line in namesTxt.readlines()]\n",
    "    names = namesTxt.read().splitlines()\n",
    "names[:8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the encoder for the letters\n",
    "chars = [\".\"] + sorted(list(set(\"\".join(names)))) \n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for i,c in enumerate(chars)}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "# define the block size of the model\n",
    "block_size = 3 # context length: how many characters to we consider to predict the next one\n",
    "\n",
    "# helper function to encode the datasets\n",
    "def buildDatasets(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in word + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    return (torch.tensor(X),torch.tensor(Y))\n",
    "\n",
    "# shuffle the order of the names\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "\n",
    "# build the datasets\n",
    "xtrain, ytrain = buildDatasets(names[:n1]) # 80%\n",
    "xtest, ytest = buildDatasets(names[n1:n2]) # 10%\n",
    "xdev, ydev = buildDatasets(names[n2:]) # 10 %"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in: int, fan_out: int, gen:torch.Generator, biais = True):\n",
    "        self.weight = torch.randn(size=(fan_in, fan_out), generator = gen) / (fan_in**0.5)\n",
    "        self.biais = torch.ones(fan_out) if biais else None\n",
    "        \n",
    "    def __call__(self, x:torch.Tensor):\n",
    "        result =  x @ self.weight\n",
    "        if self.biais is not None:\n",
    "            result += result + self.biais\n",
    "        return result\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.biais is None else [self.biais])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, num_features: int, eps = 10**-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backpropagation)\n",
    "        self.gamma = torch.ones(size=num_features)\n",
    "        self.beta = torch.zeros(size=num_features)\n",
    "        # buffers (parameters used for inferenced, without a gradient)\n",
    "        self.running_mean = torch.zeros(size=num_features)\n",
    "        self.running_var = torch.ones(size=num_features)\n",
    "        \n",
    "    def __call__(self, x:torch.Tensor):\n",
    "        \n",
    "        # forward pass\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            var = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        # update the buffers (if training)\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum)*self.running_mean + self.momentum * mean\n",
    "                self.running_var = (1-self.momentum)*self.running_var + self.momentum * var\n",
    "        \n",
    "        # normalize\n",
    "        xhat = (x - mean)/(var + self.eps)**0.5\n",
    "        \n",
    "        # scale and shift\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    \n",
    "    def __call__(self, x:torch.Tensor):\n",
    "        self.out = torch.tanh(input=x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46497\n"
     ]
    }
   ],
   "source": [
    "# Define the variable of the model\n",
    "n_emb = 10\n",
    "n_hidden = 100\n",
    "block_size = 3\n",
    "g = torch.Generator().manual_seed(SEED) \n",
    "\n",
    "# Define the Model\n",
    "C = torch.randn(size=(vocab_size, n_emb), generator=g)\n",
    "layers = [\n",
    "\tLinear(fan_in=(n_emb * block_size), fan_out=n_hidden, gen=g),\n",
    "\tTanh(),\n",
    " \tLinear(fan_in=n_hidden, fan_out=n_hidden, gen=g),\n",
    "\tTanh(),\n",
    " \tLinear(fan_in=n_hidden, fan_out=n_hidden, gen=g),\n",
    "\tTanh(),\n",
    " \tLinear(fan_in=n_hidden, fan_out=n_hidden, gen=g),\n",
    "\tTanh(),\n",
    " \tLinear(fan_in=n_hidden, fan_out=n_hidden, gen=g),\n",
    "\tTanh(),\n",
    " \tLinear(fan_in=n_hidden, fan_out=vocab_size, gen=g),\n",
    "]\n",
    "\n",
    "# fine tune initialization:\n",
    "with torch.no_grad():\n",
    "\t# make the output layer less confident\n",
    "\tlayers[-1].weight *= 0.1\n",
    "\n",
    "\t# add some gain to the hidden layers\n",
    "\tfor layer in layers[:-1]:\n",
    "\t\tif isinstance(layer, Linear):\n",
    "\t\t\tlayer.weight *= (5/3)\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "# add gradient\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "max_path = 1\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "high_train = xtrain.shape[0]\n",
    "\n",
    "for i in range(max_path):\n",
    "    # construct the minibatch\n",
    "    ix = torch.randint(low=0, high=high_train, size=(batch_size,), generator=g)\n",
    "    x_batch, y_batch = xtrain[ix], ytrain[ix]\n",
    "\n",
    "    # forward pass\n",
    "    x = C[x_batch].view((batch_size, n_emb * block_size)) # embed the characters into vectors and concatenate them shape = (32,30)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(input=x, target=y_batch)\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    lr = 0.1 if i < 10000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    # if i%10000 == 0:\n",
    "    #     print(f\"Loss of batch {i} = {loss.item()}\")\n",
    "\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Distribution\n",
    "<!-- <br> -->\n",
    "The goal hier is to have a distribution which is not too saturated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1: <class '__main__.Tanh'>, mean = 0.22, std = 0.84\n",
      "layer 3: <class '__main__.Tanh'>, mean = 0.23, std = 0.83\n",
      "layer 5: <class '__main__.Tanh'>, mean = 0.21, std = 0.84\n",
      "layer 7: <class '__main__.Tanh'>, mean = 0.32, std = 0.82\n",
      "layer 9: <class '__main__.Tanh'>, mean = 0.25, std = 0.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "legends = []\n",
    "\n",
    "for i, layer in enumerate(layers):\n",
    "    if isinstance(layer, Tanh):\n",
    "        l_out = layer.out\n",
    "        print(f\"layer {i}: {layer.__class__}, mean = {l_out.mean():.2f}, std = {l_out.std():.2f}, saturated = {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5171875"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([p for l in layers[1].out for p in l if abs(p) >0.97])/len([p for l in layers[1].out for p in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1683b6920>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the MLP = 12097\n"
     ]
    }
   ],
   "source": [
    "# define the parameters of the MLP\n",
    "n_embd = 10 # the dimentionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of hidden layers\n",
    "g = torch.Generator().manual_seed(SEED) \n",
    "\n",
    "# initialization\n",
    "C = torch.randn(size = (vocab_size, n_embd), generator = g) \n",
    "w1 = torch.randn(size=(n_embd * block_size, n_hidden), generator = g)\n",
    "w1 = w1 * ((5/3) / (n_embd * block_size)**0.5) # normalize for the tanh activation\n",
    "w2 = torch.randn(size=(n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.randn(size=(vocab_size,), generator=g) * 0\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbiais = torch.zeros((1,n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, w1, w2, b2, bngain, bnbiais]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(f\"Number of parameters of the MLP = {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss of the batch Nr.0 = 3.3239\n",
      "The loss of the batch Nr.10000 = 2.0322\n",
      "The loss of the batch Nr.20000 = 2.5675\n",
      "The loss of the batch Nr.30000 = 2.0125\n",
      "The loss of the batch Nr.40000 = 2.2446\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m bnstdi \u001b[39m=\u001b[39m hpreact\u001b[39m.\u001b[39mstd(\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m# the standard deviation of this batch\u001b[39;00m\n\u001b[1;32m     25\u001b[0m hpreact \u001b[39m=\u001b[39m bngain \u001b[39m*\u001b[39m ((hpreact \u001b[39m-\u001b[39m bnmeani)\u001b[39m/\u001b[39m(bnstdi)) \u001b[39m+\u001b[39m bnbiais \u001b[39m# normalize the batch\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mno_grad():\n\u001b[1;32m     27\u001b[0m \tbnmean_running \u001b[39m=\u001b[39m \u001b[39m0.999\u001b[39m \u001b[39m*\u001b[39m bnmean_running \u001b[39m+\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m bnmeani \u001b[39m# update estimate of global mean\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \tbnstd_running \u001b[39m=\u001b[39m \u001b[39m0.999\u001b[39m \u001b[39m*\u001b[39m bnstd_running \u001b[39m+\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m bnstdi \u001b[39m# update estimate of global standard deviation\u001b[39;00m\n",
      "File \u001b[0;32m~/Data_Science/Projects/makemore/.venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:126\u001b[0m, in \u001b[0;36mno_grad.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mno_grad\u001b[39;00m(_DecoratorContextManager):\n\u001b[1;32m     88\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Context-manager that disabled gradient calculation.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[39m    Disabling gradient calculation is useful for inference, when you are sure\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m        False\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_jit_internal\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    128\u001b[0m             \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "max_path = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "xtrain_len = xtrain.shape[0]\n",
    "\n",
    "# forward pass\n",
    "for i in range(max_path):\n",
    "    \n",
    "\t# prepare the data of the training batch\n",
    "\tix = torch.randint(low = 0, high=xtrain_len, size=(batch_size,), generator=g)\n",
    "\txbatch, ybatch = xtrain[ix], ytrain[ix]\n",
    "\n",
    "\t# forward pass\n",
    "\t# Tensor embedding\n",
    "\temb = C[xbatch] \n",
    "\tembcat = emb.view(emb.shape[0], -1) # concatenate the tensors\n",
    "\t\n",
    "\t# first hidden layer\n",
    "\thpreact = embcat @ w1 # pass through the first hidden layer\n",
    "\t\n",
    "\t# batch normalization layer\n",
    "\tbnmeani = hpreact.mean(0, keepdim=True) # the mean of this batch\n",
    "\tbnstdi = hpreact.std(0, keepdim=True) # the standard deviation of this batch\n",
    "\thpreact = bngain * ((hpreact - bnmeani)/(bnstdi)) + bnbiais # normalize the batch\n",
    "\twith torch.no_grad():\n",
    "\t\tbnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani # update estimate of global mean\n",
    "\t\tbnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi # update estimate of global standard deviation\n",
    "\t\n",
    "\t# activation normalized layer\n",
    "\th = torch.tanh(hpreact)\n",
    "\t\n",
    "\t# output\n",
    "\tlogits = h @ w2 + b2 # output layer\n",
    " \n",
    "\t# loss\n",
    "\tloss = F.cross_entropy(logits, ybatch) # loss function of this batch\n",
    "\n",
    "\t# backward pass\n",
    "\tfor p in parameters:\n",
    "\t\tp.grad = None\n",
    "\tloss.backward()\n",
    "\n",
    "\t# update the weights\n",
    "\tlr = 0.1 if i<50000 else 0.01\n",
    "\tfor p in parameters:\n",
    "\t\tp.data += - lr * p.grad\n",
    "  \n",
    "\t# print the results once in a while\n",
    "\tif i % 10000 == 0:\n",
    "\t\tprint(f\"The loss of the batch Nr.{i} = {loss.item():.4f}\")\n",
    "\tlossi.append(loss.log10().item())\n",
    " \n",
    "\t# break\n",
    "\n",
    "# print the evolution of the loss\n",
    "plt.plot(lossi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to calculate the loss of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 2.0943093299865723\n",
      "dev loss = 2.1223857402801514\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # disable the operation gradient tracking\n",
    "def print_loss(dataset):\n",
    "\tdic = {\n",
    "\t\t\"train\": (xtrain, ytrain),\n",
    "\t\t\"dev\": (xdev, ydev),\n",
    "\t\t\"test\": (xtest, ytest)\n",
    "\t}\n",
    "\tX, Y = dic[dataset]\n",
    "\temb = C[X] # embed the characters into tensor\n",
    "\tembcat = emb.view(emb.shape[0], -1) # concatenate the tensors\n",
    "\thpreact = embcat @ w1\n",
    "\thpreact = bngain * ((hpreact - bnmean_running)/(bnstd_running)) + bnbiais\n",
    "\th = torch.tanh(hpreact) # activation function of the first layer\n",
    "\tlogits = h @ w2 + b2 # output layer\n",
    "\tloss = F.cross_entropy(logits, Y) # loss function of this batch\n",
    "\tprint(f\"{dataset} loss = {loss}\")\n",
    "print_loss(\"train\")\n",
    "print_loss(\"dev\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict new names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaym\n",
      "kayha\n",
      "kham\n",
      "alfiaa\n",
      "bvettini\n",
      "soley\n",
      "cadyn\n",
      "dilli\n",
      "evin\n",
      "kann\n"
     ]
    }
   ],
   "source": [
    "# predict names\n",
    "newNames = []\n",
    "for i in range (10):\n",
    "\tnewName = \"\"\n",
    "\tcontext = [0] * block_size\n",
    "\twhile True:\n",
    "\t\temb = C[context] # embed the characters into tensor\n",
    "\t\tembcat = emb.view(30) # concatenate the tensors\n",
    "\t\thpreact = embcat @ w1 # pass through the first hidden layer\n",
    "\t\thpreact = bngain * ((hpreact - bnmean_running)/(bnstd_running)) + bnbiais\n",
    "\t\th = torch.tanh(hpreact) # activation function of the first layer\n",
    "\t\tlogits = h @ w2 + b2 # output layer\n",
    "\t\tproba = logits.exp() / logits.exp().sum()\n",
    "\n",
    "\t\ti_out = torch.multinomial(\n",
    "\t\t\tinput=proba,\n",
    "\t\t\tnum_samples=1,\n",
    "\t\t\treplacement=True,\n",
    "\t\t\tgenerator=g,\n",
    "\t\t).item()\n",
    "\n",
    "\t\tcontext = context[1:] + [i_out]\n",
    "\t\tnewName += itos[i_out]\n",
    "\n",
    "\t\tif i_out == 0:\n",
    "\t\t\tbreak\t\t\n",
    "\tnewNames.append(newName)\n",
    "\n",
    "for name in newNames:\n",
    "    print(name[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8398e49975ed50dac93819d6da1fdcd2c3c7f69ed96c4ef7eb3607333daa0138"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
